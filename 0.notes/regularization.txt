Early termination
= stopping as validation set performance starts dropping after rising with epochs, because that means that the net is starting to overfit

Regularisation
L2 = adding a reg term to loss fct
beta * ||W||2

-> simple
-> no need to change network structure

Dropout = randomly turning off half the activation flowing through the net for each training example 
=> forces the net to learn redundant representations
=> robust nets, prevents overfitting
=> net acts as consensus of multiple sub nets

Need to scale remaining activations during training so that network acts normally during eval

tf.nn.dropout()